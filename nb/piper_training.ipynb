{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvjob/colab.google/blob/main/nb/piper_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "form",
        "id": "qyxSMuzjfQrz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "47565793-a6d1-4748-ad7a-71fdbf7458db"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "function ClickConnect(){\n",
              "console.log(\"Working\");\n",
              "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
              "}\n",
              "setInterval(ClickConnect,60000)\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Google Colab Anti-Disconnect\n",
        "\n",
        "import IPython\n",
        "js_code = '''\n",
        "function ClickConnect(){\n",
        "console.log(\"Working\");\n",
        "document.querySelector(\"colab-toolbar-button#connect\").click()\n",
        "}\n",
        "setInterval(ClickConnect,60000)\n",
        "'''\n",
        "display(IPython.display.Javascript(js_code))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "form",
        "id": "ygxzp-xHTC7T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a6104d0-2928-4929-e3fb-e20893a3b2a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Mar 22 13:29:10 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   49C    P8              9W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# Check GPU type\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Google drive connect\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "YaGev1MBBs8P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240b2e6d-1ed8-4dd1-9d00-7dd438f84efc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "_XwmTVlcUgCh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "723312df-63cb-4903-8d0a-2c9582edeac4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/piper/src/python\n",
            "\u001b[33mWARNING: Ignoring version 1.7.0 of pytorch-lightning since it has invalid metadata:\n",
            "Requested pytorch-lightning==1.7.0 from https://files.pythonhosted.org/packages/2f/07/57fd85991eec4311b9945369158bca1322f801686d701322a65789852d46/pytorch_lightning-1.7.0-py3-none-any.whl has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    torch (>=1.9.*)\n",
            "           ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement pytorch-lightning==1.7.0 (from versions: 0.0.2, 0.2, 0.2.2, 0.2.3, 0.2.4, 0.2.4.1, 0.2.5, 0.2.5.1, 0.2.5.2, 0.2.6, 0.3, 0.3.1, 0.3.2, 0.3.3, 0.3.4, 0.3.4.1, 0.3.5, 0.3.6, 0.3.6.1, 0.3.6.3, 0.3.6.4, 0.3.6.5, 0.3.6.6, 0.3.6.7, 0.3.6.8, 0.3.6.9, 0.4.0, 0.4.1, 0.4.2, 0.4.3, 0.4.4, 0.4.5, 0.4.6, 0.4.7, 0.4.8, 0.4.9, 0.5.0, 0.5.1, 0.5.1.2, 0.5.1.3, 0.5.2, 0.5.2.1, 0.5.3, 0.5.3.1, 0.5.3.2, 0.5.3.3, 0.6.0, 0.7.1, 0.7.3, 0.7.5, 0.7.6, 0.8.1, 0.8.3, 0.8.4, 0.8.5, 0.9.0, 0.10.0, 1.0.0, 1.0.1, 1.0.2, 1.0.3, 1.0.4, 1.0.5, 1.0.6, 1.0.7, 1.0.8, 1.1.0, 1.1.1, 1.1.2, 1.1.3, 1.1.4, 1.1.5, 1.1.6, 1.1.7, 1.1.8, 1.2.0rc0, 1.2.0rc1, 1.2.0rc2, 1.2.0, 1.2.1, 1.2.2, 1.2.3, 1.2.4, 1.2.5, 1.2.6, 1.2.7, 1.2.8, 1.2.9, 1.2.10, 1.3.0rc1, 1.3.0rc2, 1.3.0rc3, 1.3.0, 1.3.1, 1.3.2, 1.3.3, 1.3.4, 1.3.5, 1.3.6, 1.3.7, 1.3.7.post0, 1.3.8, 1.4.0rc0, 1.4.0rc1, 1.4.0rc2, 1.4.0, 1.4.1, 1.4.2, 1.4.3, 1.4.4, 1.4.5, 1.4.6, 1.4.7, 1.4.8, 1.4.9, 1.5.0rc0, 1.5.0rc1, 1.5.0, 1.5.1, 1.5.2, 1.5.3, 1.5.4, 1.5.5, 1.5.6, 1.5.7, 1.5.8, 1.5.9, 1.5.10, 1.5.10.post0, 1.6.0rc0, 1.6.0rc1, 1.6.0, 1.6.1, 1.6.2, 1.6.3, 1.6.4, 1.6.5, 1.6.5.post0, 1.7.0rc0, 1.7.0rc1, 1.7.0, 1.7.1, 1.7.2, 1.7.3, 1.7.4, 1.7.5, 1.7.6, 1.7.7, 1.8.0rc0, 1.8.0rc1, 1.8.0rc2, 1.8.0, 1.8.0.post1, 1.8.1, 1.8.2, 1.8.3, 1.8.3.post0, 1.8.3.post1, 1.8.3.post2, 1.8.4, 1.8.4.post0, 1.8.5, 1.8.5.post0, 1.8.6, 1.9.0rc0, 1.9.0, 1.9.1, 1.9.2, 1.9.3, 1.9.4, 1.9.5, 2.0.0rc0, 2.0.0, 2.0.1, 2.0.1.post0, 2.0.2, 2.0.3, 2.0.4, 2.0.5, 2.0.6, 2.0.7, 2.0.8, 2.0.9, 2.0.9.post0, 2.1.0rc0, 2.1.0rc1, 2.1.0, 2.1.1, 2.1.2, 2.1.3, 2.1.4, 2.2.0rc0, 2.2.0, 2.2.0.post0, 2.2.1, 2.2.2, 2.2.3, 2.2.4, 2.2.5, 2.3.0, 2.3.1, 2.3.2, 2.3.3, 2.4.0, 2.5.0rc0, 2.5.0, 2.5.0.post0, 2.5.1rc0, 2.5.1rc1, 2.5.1rc2, 2.5.1)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pytorch-lightning==1.7.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchtext==0.12.0 (from versions: 0.1.1, 0.2.0, 0.2.1, 0.2.3, 0.3.1, 0.4.0, 0.5.0, 0.6.0, 0.15.1, 0.15.2, 0.16.0, 0.16.1, 0.16.2, 0.17.0, 0.17.1, 0.17.2, 0.18.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchtext==0.12.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Ignored the following yanked versions: 2.0.0\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio==0.11.0 (from versions: 2.0.1, 2.0.2, 2.1.0, 2.1.1, 2.1.2, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.3.1, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio==0.11.0\u001b[0m\u001b[31m\n",
            "\u001b[0mReading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "espeak-ng is already the newest version (1.50+dfsg-10ubuntu0.1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 29 not upgraded.\n",
            "/content\n"
          ]
        }
      ],
      "source": [
        "# Install software\n",
        "\n",
        "!git clone -q https://github.com/rmcpantoja/piper\n",
        "%cd /content/piper/src/python\n",
        "!wget -q \"https://raw.githubusercontent.com/coqui-ai/TTS/dev/TTS/bin/resample.py\"\n",
        "!pip install -q -r requirements.txt\n",
        "!pip install -q cython>=0.29.0 piper-phonemize==1.1.0 librosa>=0.9.2 numpy>=1.19.0 onnxruntime>=1.11.0 pytorch-lightning==1.7.0 torch==1.11.0\n",
        "!pip install -q torchtext==0.12.0 torchvision==0.12.0\n",
        "!pip install -q torchaudio==0.11.0 torchmetrics==0.11.4\n",
        "!bash build_monotonic_align.sh\n",
        "!apt-get install -q espeak-ng\n",
        "!gdown -q \"1EWEb7amo1rgFGpBFfRD4BKX3pkjVK1I-\" -O \"/content/piper/src/python/patch.zip\"\n",
        "!unzip -o -q \"patch.zip\"\n",
        "%cd /content"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "config-cell"
      },
      "outputs": [],
      "source": [
        "# Configuration\n",
        "config = {\n",
        "    \"language\": \"ru\",\n",
        "    \"single_speaker\": True,\n",
        "    \"dataset\": \"/content/drive/MyDrive/dataset\",\n",
        "    \"output\": \"/content/output\",\n",
        "    \"resume_from_checkpoint\": \"/content/drive/MyDrive/irina_baba.ckpt\",\n",
        "    \"quality\": \"medium\",\n",
        "    \"max_epochs\": 5000,\n",
        "    \"batch_size\": 20,\n",
        "    \"checkpoint_epochs\": 100,\n",
        "    \"max_phoneme_ids\": 800,\n",
        "    \"resample\": False,\n",
        "    \"sample_rate\": \"22050\",\n",
        "    \"validation_split\": 0.0,\n",
        "    \"num_test_examples\": 0,\n",
        "    \"dataset_format\": \"ljspeech\",\n",
        "    \"precision\": 32\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "cellView": "form",
        "id": "dOyx9Y6JYvRF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0be7045-fce3-4737-e13a-fda83c7510de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/piper/src/python\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/piper/src/python/piper_train/preprocess.py\", line 17, in <module>\n",
            "    from piper_phonemize import (\n",
            "ModuleNotFoundError: No module named 'piper_phonemize'\n"
          ]
        }
      ],
      "source": [
        "# Preprocess dataset\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists(config['output']):\n",
        "  os.makedirs(config['output'])\n",
        "\n",
        "force_sp = \" --single-speaker\" if config['single_speaker'] else \"\"\n",
        "\n",
        "%cd /content/piper/src/python\n",
        "\n",
        "if config['resample']:\n",
        "  !python resample.py --input_dir \"/content/dataset/wav\" --output_dir \"/content/dataset/wavs_resampled\" --output_sr {config['sample_rate']} --file_ext \"wav\"\n",
        "  !mv /content/dataset/wavs_resampled/* /content/dataset/wavs\n",
        "\n",
        "!python -m piper_train.preprocess \\\n",
        "  --language {config['language']} \\\n",
        "  --input-dir {config['dataset']} \\\n",
        "  --output-dir {config['output']} \\\n",
        "  --dataset-format {config['dataset_format']} \\\n",
        "  --sample-rate {config['sample_rate']} \\\n",
        "  --max-workers 4 \\\n",
        "  {force_sp}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "background_save": true
        },
        "id": "X4zbSjXg2J3N"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "\n",
        "get_ipython().system(f'''\n",
        "python -m piper_train \\\n",
        "--dataset-dir {config['output']} \\\n",
        "--accelerator 'gpu' \\\n",
        "--devices 1 \\\n",
        "--batch-size {config['batch_size']} \\\n",
        "--validation-split {config['validation_split']} \\\n",
        "--num-test-examples {config['num_test_examples']} \\\n",
        "--quality \"{config['quality']}\" \\\n",
        "--checkpoint-epochs {config['checkpoint_epochs']} \\\n",
        "--max_epochs {config['max_epochs']} \\\n",
        "--precision {config['precision']} \\\n",
        "--max-phoneme-ids {config['max_phoneme_ids']} \\\n",
        "--resume_from_checkpoint \"{config['resume_from_checkpoint']}\"\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "export-cell"
      },
      "outputs": [],
      "source": [
        "# Export model to ONNX\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Find latest checkpoint\n",
        "checkpoint_dir = os.path.join(config['output'], \"lightning_logs\", \"version_0\", \"checkpoints\")\n",
        "checkpoints = [os.path.join(checkpoint_dir, f) for f in os.listdir(checkpoint_dir) if f.endswith(\".ckpt\")]\n",
        "latest_checkpoint = max(checkpoints, key=os.path.getctime)\n",
        "\n",
        "# Export path\n",
        "export_path = os.path.join(config['output'], \"model.onnx\")\n",
        "\n",
        "!python -m piper_train.export_onnx {latest_checkpoint} {export_path}\n",
        "shutil.copy(\n",
        "    os.path.join(config['output'], \"config.json\"),\n",
        "    f\"{export_path}.json\"\n",
        ")\n",
        "\n",
        "print(f\"Model exported to {export_path}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}