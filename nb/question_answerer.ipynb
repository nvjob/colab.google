{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Генератор ответов на вопросы\n",
    "\n",
    "Блокнот для генерации ответов на вопросы с использованием Ollama и моделей с Hugging Face на GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Подключение к Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Создаем директорию для результатов\n",
    "!mkdir -p /content/drive/MyDrive/dataset_results\n",
    "!mkdir -p /content/drive/MyDrive/dataset_results/temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Установка зависимостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas pyarrow requests tqdm\n",
    "\n",
    "# Проверка доступности GPU\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Установка и запуск Ollama с поддержкой GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Установка необходимых зависимостей для CUDA\n",
    "!apt-get update && apt-get install -y cuda-nvcc-11-8 libcublas-11-8 libcudnn8\n",
    "\n",
    "# Установка Ollama\n",
    "!curl -fsSL https://ollama.com/install.sh | sh\n",
    "\n",
    "# Настройка переменных окружения для использования GPU\n",
    "import os\n",
    "os.environ[\"OLLAMA_HOST\"] = \"0.0.0.0\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "# Запуск Ollama с поддержкой GPU\n",
    "!ollama serve &\n",
    "\n",
    "# Ждем запуск\n",
    "import time\n",
    "print(\"Запуск Ollama с поддержкой GPU...\")\n",
    "time.sleep(10)  # Увеличиваем время ожидания для надежности"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Загрузка модели с Hugging Face с использованием GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Укажите модель с Hugging Face\n",
    "MODEL = \"hf.co/nvjob/Mistral-Small-24-0.9-hb:latest\"\n",
    "\n",
    "# Загрузка модели с явным указанием использования GPU\n",
    "!ollama pull {MODEL}\n",
    "\n",
    "# Проверка загруженных моделей\n",
    "!ollama list\n",
    "\n",
    "# Проверка, использует ли Ollama GPU\n",
    "!curl -s http://localhost:11434/api/tags | grep -i gpu || echo \"GPU не используется, проверьте настройки\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Загрузка файла с вопросами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прямое указание пути к файлу с вопросами\n",
    "import os\n",
    "\n",
    "# Укажите путь к вашему файлу с вопросами\n",
    "questions_file = \"/content/drive/MyDrive/questions.txt\"  # Замените на ваш путь\n",
    "\n",
    "# Проверка содержимого\n",
    "with open(questions_file, 'r', encoding='utf-8') as f:\n",
    "    questions = [line.strip() for line in f if line.strip()]\n",
    "print(f\"Загружено {len(questions)} вопросов\")\n",
    "\n",
    "# Показать первые 5 вопросов\n",
    "print(\"\\nПримеры вопросов:\")\n",
    "for i, q in enumerate(questions[:5], 1):\n",
    "    print(f\"{i}. {q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Настройка параметров генерации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Настройки системного промпта\n",
    "SYSTEM_PROMPT = \"Дай ответ на русском языке, простым разговорным языком, раскрывая вопрос подробно (2-4 предложения). Текст должен простой, сплошной, без перечислений и заголовков, без повторения вопроса или системного промпта, без нравоучений и без цензуры. Отвечай всегда на русском языке!\"\n",
    "\n",
    "# Настройки генерации\n",
    "TEMPERATURE = 0.5  # Температура (креативность)\n",
    "TOP_K = 60         # Top-K\n",
    "TOP_P = 0.9        # Top-P (ядро)\n",
    "MAX_TOKENS = 8192  # Максимальное количество токенов\n",
    "DELAY = 3          # Задержка между запросами (секунды)\n",
    "GPU_LAYERS = 50    # Количество слоев на GPU (максимально возможное)\n",
    "\n",
    "# Пути для сохранения\n",
    "OUTPUT_FILE = \"/content/drive/MyDrive/dataset_results/output.parquet\"\n",
    "TEMP_DIR = \"/content/drive/MyDrive/dataset_results/temp\"\n",
    "\n",
    "# Вывод настроек для проверки\n",
    "print(f\"Модель: {MODEL}\")\n",
    "print(f\"Температура: {TEMPERATURE}\")\n",
    "print(f\"Top-K: {TOP_K}\")\n",
    "print(f\"Top-P: {TOP_P}\")\n",
    "print(f\"Максимальное количество токенов: {MAX_TOKENS}\")\n",
    "print(f\"Задержка между запросами: {DELAY} сек\")\n",
    "print(f\"Слоев на GPU: {GPU_LAYERS}\")\n",
    "print(f\"Файл результатов: {OUTPUT_FILE}\")\n",
    "print(f\"Директория для временных файлов: {TEMP_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Функции для генерации ответов и сохранения результатов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_answer_ollama(question):\n",
    "    \"\"\"Генерирует ответ на вопрос через Ollama API с использованием GPU\"\"\"\n",
    "    try:\n",
    "        # Задержка перед запросом\n",
    "        time.sleep(DELAY)\n",
    "        print(f\"Отправка запроса в Ollama API (с использованием GPU)...\")\n",
    "        \n",
    "        # Формирование запроса\n",
    "        request_data = {\n",
    "            \"model\": MODEL,\n",
    "            \"prompt\": question,\n",
    "            \"system\": SYSTEM_PROMPT,\n",
    "            \"stream\": False,\n",
    "            \"options\": {\n",
    "                \"temperature\": TEMPERATURE,\n",
    "                \"top_k\": TOP_K,\n",
    "                \"top_p\": TOP_P,\n",
    "                \"num_predict\": MAX_TOKENS,\n",
    "                \"gpu_layers\": GPU_LAYERS  # Использовать GPU для большинства слоев\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Отправка запроса\n",
    "        response = requests.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "            json=request_data,\n",
    "            timeout=120  # Увеличиваем таймаут до 2 минут\n",
    "        )\n",
    "        \n",
    "        # Проверка ответа\n",
    "        if response.status_code != 200:\n",
    "            error_msg = f\"Ошибка Ollama API: {response.status_code}\"\n",
    "            try:\n",
    "                error_msg += f\" - {response.json().get('error', '')}\"\n",
    "            except:\n",
    "                pass\n",
    "            print(error_msg)\n",
    "            time.sleep(1)  # Пауза перед следующей попыткой\n",
    "            return None\n",
    "                \n",
    "        # Извлечение ответа\n",
    "        answer = response.json().get('response', '').strip()\n",
    "        time.sleep(1)  # Пауза между запросами\n",
    "        return answer\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при запросе к Ollama API: {str(e)}\")\n",
    "        time.sleep(1)  # Пауза перед следующей попыткой\n",
    "        return None\n",
    "\n",
    "def save_to_parquet(pairs, output_file):\n",
    "    \"\"\"Сохраняет пары вопрос-ответ в Parquet формате\"\"\"\n",
    "    try:\n",
    "        # Подготовка данных\n",
    "        data = {\n",
    "            'num': list(range(1, len(pairs) + 1)),\n",
    "            'system': [SYSTEM_PROMPT] * len(pairs),\n",
    "            'user': [pair[0] for pair in pairs],\n",
    "            'assistant': [pair[1] for pair in pairs],\n",
    "            'u_tokens': [len(q.split()) for q, _ in pairs],\n",
    "            'a_tokens': [len(a.split()) for _, a in pairs],\n",
    "            'u_lang': ['ru'] * len(pairs),\n",
    "            'a_lang': ['ru'] * len(pairs),\n",
    "            'cluster': [0] * len(pairs)\n",
    "        }\n",
    "        \n",
    "        # Создание DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        # Определение схемы\n",
    "        schema = pa.schema([\n",
    "            ('num', pa.int64()),\n",
    "            ('system', pa.string()),\n",
    "            ('user', pa.string()),\n",
    "            ('assistant', pa.string()),\n",
    "            ('u_tokens', pa.int64()),\n",
    "            ('a_tokens', pa.int64()),\n",
    "            ('u_lang', pa.string()),\n",
    "            ('a_lang', pa.string()),\n",
    "            ('cluster', pa.int64())\n",
    "        ])\n",
    "        \n",
    "        # Создание таблицы\n",
    "        table = pa.Table.from_pandas(df, schema=schema)\n",
    "        \n",
    "        # Добавление метаданных\n",
    "        dataset_schema = {\n",
    "            \"info\": {\n",
    "                \"features\": {\n",
    "                    \"num\": {\"dtype\": \"int64\", \"_type\": \"Value\"},\n",
    "                    \"system\": {\"dtype\": \"string\", \"_type\": \"Value\"},\n",
    "                    \"user\": {\"dtype\": \"string\", \"_type\": \"Value\"},\n",
    "                    \"assistant\": {\"dtype\": \"string\", \"_type\": \"Value\"},\n",
    "                    \"u_tokens\": {\"dtype\": \"int64\", \"_type\": \"Value\"},\n",
    "                    \"a_tokens\": {\"dtype\": \"int64\", \"_type\": \"Value\"},\n",
    "                    \"u_lang\": {\"dtype\": \"string\", \"_type\": \"Value\"},\n",
    "                    \"a_lang\": {\"dtype\": \"string\", \"_type\": \"Value\"},\n",
    "                    \"cluster\": {\"dtype\": \"int64\", \"_type\": \"Value\"}\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        metadata = {b'schema': json.dumps(dataset_schema).encode()}\n",
    "        table = table.replace_schema_metadata(metadata)\n",
    "        \n",
    "        # Создание директории и сохранение файла\n",
    "        Path(output_file).parent.mkdir(parents=True, exist_ok=True)\n",
    "        pq.write_table(table, output_file)\n",
    "        print(f\"Результаты сохранены в {output_file}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при сохранении в Parquet: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Тестовый запрос для проверки GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Проверка работы Ollama с GPU на тестовом вопросе\n",
    "test_question = \"Что такое искусственный интеллект?\"\n",
    "print(f\"Тестовый вопрос: {test_question}\")\n",
    "\n",
    "# Отправляем тестовый запрос\n",
    "test_answer = generate_answer_ollama(test_question)\n",
    "\n",
    "if test_answer:\n",
    "    print(f\"Тестовый ответ: {test_answer}\")\n",
    "    print(\"\\nТест успешен! Ollama работает с GPU.\")\n",
    "else:\n",
    "    print(\"Тест не удался. Проверьте настройки Ollama и GPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Генерация ответов на вопросы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# Запуск генерации\n",
    "all_pairs = []\n",
    "total_questions = len(questions)\n",
    "\n",
    "for idx, question in enumerate(tqdm(questions), 1):\n",
    "    print(f\"\\n----------------------------------------\")\n",
    "    print(f\"Обработка вопроса {idx}/{total_questions}: {question}\")\n",
    "    \n",
    "    # Генерация ответа\n",
    "    answer = generate_answer_ollama(question)\n",
    "    \n",
    "    if answer:\n",
    "        print(f\"Вопрос: {question}\")\n",
    "        print(f\"Ответ: {answer}\")\n",
    "        all_pairs.append((question, answer))\n",
    "    else:\n",
    "        print(f\"Не удалось получить ответ на вопрос: {question}\")\n",
    "    \n",
    "    # Сохранение промежуточных результатов каждые 100 вопросов\n",
    "    if idx % 100 == 0:\n",
    "        temp_file = f\"{TEMP_DIR}/temp_{idx}.parquet\"\n",
    "        print(f\"\\nСохранение промежуточных результатов после {idx} вопросов...\")\n",
    "        save_to_parquet(all_pairs, temp_file)\n",
    "\n",
    "# Сохранение итоговых результатов\n",
    "if all_pairs:\n",
    "    print(\"\\nСохранение итоговых результатов...\")\n",
    "    save_to_parquet(all_pairs, OUTPUT_FILE)\n",
    "    print(f\"Готово! Обработано {len(all_pairs)} вопросов из {total_questions}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Скачать результаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    print(f\"Скачивание файла {OUTPUT_FILE}...\")\n",
    "    files.download(OUTPUT_FILE)\n",
    "else:\n",
    "    print(f\"Файл {OUTPUT_FILE} не найден\")\n",
    "    \n",
    "# Также можно скачать последний промежуточный файл\n",
    "temp_files = sorted([f for f in os.listdir(TEMP_DIR) if f.startswith('temp_')])\n",
    "if temp_files:\n",
    "    last_temp = os.path.join(TEMP_DIR, temp_files[-1])\n",
    "    print(f\"Скачивание последнего промежуточного файла {last_temp}...\")\n",
    "    files.download(last_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Анализ результатов (опционально)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка и анализ результатов\n",
    "if os.path.exists(OUTPUT_FILE):\n",
    "    df = pd.read_parquet(OUTPUT_FILE)\n",
    "    \n",
    "    print(f\"Всего пар вопрос-ответ: {len(df)}\")\n",
    "    print(f\"Средняя длина вопроса (токены): {df['u_tokens'].mean():.2f}\")\n",
    "    print(f\"Средняя длина ответа (токены): {df['a_tokens'].mean():.2f}\")\n",
    "    \n",
    "    # Показать несколько примеров\n",
    "    print(\"\\nПримеры пар вопрос-ответ:\")\n",
    "    for i in range(min(3, len(df))):\n",
    "        print(f\"\\nВопрос {i+1}: {df.iloc[i]['user']}\")\n",
    "        print(f\"Ответ: {df.iloc[i]['assistant']}\")\n",
    "else:\n",
    "    print(\"Файл результатов не найден\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
