{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nvjob/colab.google/blob/main/nb/Gemma3_(1B).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CIgnpkjdw7-V"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    !pip install --no-deps unsloth vllm\n",
        "# Install latest Hugging Face for Gemma-3!\n",
        "!pip install --no-deps git+https://github.com/huggingface/transformers@v4.49.0-Gemma-3"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import os\n",
        "if \"COLAB_\" not in \"\".join(os.environ.keys()):\n",
        "    !pip install unsloth vllm\n",
        "else:\n",
        "    !pip install --no-deps unsloth vllm\n",
        "    # [NOTE] Do the below ONLY in Colab! Use [[pip install unsloth vllm]]\n",
        "    # Skip restarting message in Colab\n",
        "    import sys, re, requests; modules = list(sys.modules.keys())\n",
        "    for x in modules: sys.modules.pop(x) if \"PIL\" in x or \"google\" in x else None\n",
        "    !pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy unsloth_zoo\n",
        "    !pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "\n",
        "    # vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
        "    f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
        "    with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
        "        file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
        "    !pip install -r vllm_requirements.txt"
      ],
      "metadata": {
        "id": "h3kyJp5uw87m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Xbb0cuLzwgf"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastModel\n",
        "import torch\n",
        "\n",
        "fourbit_models = [\n",
        "    # 4bit dynamic quants for superior accuracy and low memory use\n",
        "    \"unsloth/gemma-3-1b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n",
        "    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n",
        "\n",
        "    # Other popular models!\n",
        "    \"unsloth/Llama-3.1-8B\",\n",
        "    \"unsloth/Llama-3.2-3B\",\n",
        "    \"unsloth/Llama-3.3-70B\",\n",
        "    \"unsloth/mistral-7b-instruct-v0.3\",\n",
        "    \"unsloth/Phi-4\",\n",
        "] # More models at https://huggingface.co/unsloth\n",
        "\n",
        "model, tokenizer = FastModel.from_pretrained(\n",
        "    model_name = \"unsloth/gemma-3-1b-it\",\n",
        "    max_seq_length = 2048, # Choose any for long context!\n",
        "    load_in_4bit = True,  # 4 bit quantization to reduce memory\n",
        "    load_in_8bit = False, # [NEW!] A bit more accurate, uses 2x memory\n",
        "    full_finetuning = False, # [NEW!] We have full finetuning now!\n",
        "    # token = \"hf_...\", # use one if using gated models\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bZsfBuZDeCL"
      },
      "outputs": [],
      "source": [
        "model = FastModel.get_peft_model(\n",
        "    model,\n",
        "    finetune_vision_layers     = False, # Turn off for just text!\n",
        "    finetune_language_layers   = True,  # Should leave on!\n",
        "    finetune_attention_modules = True,  # Attention good for GRPO\n",
        "    finetune_mlp_modules       = True,  # SHould leave on always!\n",
        "\n",
        "    r = 8,           # Larger = higher accuracy, but might overfit\n",
        "    lora_alpha = 8,  # Recommended alpha == r at least\n",
        "    lora_dropout = 0,\n",
        "    bias = \"none\",\n",
        "    random_state = 3407,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjY75GoYUCB8"
      },
      "outputs": [],
      "source": [
        "from unsloth.chat_templates import get_chat_template\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    chat_template = \"gemma-3\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mkq4RvEq7FQr"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"mastuban1/nvqa\", split = \"train\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dzE1OEXi7s3P"
      },
      "outputs": [],
      "source": [
        "dataset[100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ahE8Ys37JDJ"
      },
      "outputs": [],
      "source": [
        "def format_conversation(example):\n",
        "    # Создаем список сообщений в формате \"user\" и \"model\"\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": example[\"question\"]},\n",
        "        {\"role\": \"model\", \"content\": example[\"answer\"]}\n",
        "    ]\n",
        "    return {\"messages\": messages}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGFzmplrEy9I"
      },
      "outputs": [],
      "source": [
        "dataset = dataset.map(format_conversation)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "# Загружаем токенизатор (замените на путь к вашему токенизатору)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"unsloth/gemma-3-1b-it\")\n",
        "\n",
        "# Добавляем шаблон в токенизатор\n",
        "tokenizer.chat_template = \"\"\"\n",
        "{{ bos_token }}\n",
        "{%- if messages[0]['role'] == 'system' -%}\n",
        "    {%- if messages[0]['content'] is string -%}\n",
        "        {%- set first_user_prefix = messages[0]['content'] + '\\n\\n' -%}\n",
        "    {%- else -%}\n",
        "        {%- set first_user_prefix = messages[0]['content'][0]['text'] + '\\n\\n' -%}\n",
        "    {%- endif -%}\n",
        "    {%- set loop_messages = messages[1:] -%}\n",
        "{%- else -%}\n",
        "    {%- set first_user_prefix = \"\" -%}\n",
        "    {%- set loop_messages = messages -%}\n",
        "{%- endif -%}\n",
        "{%- for message in loop_messages -%}\n",
        "    {%- if (message['role'] == 'user') != (loop.index0 % 2 == 0) -%}\n",
        "        {{ raise_exception(\"Conversation roles must alternate user/assistant/user/assistant/...\") }}\n",
        "    {%- endif -%}\n",
        "    {%- if (message['role'] == 'assistant') -%}\n",
        "        {%- set role = \"model\" -%}\n",
        "    {%- else -%}\n",
        "        {%- set role = message['role'] -%}\n",
        "    {%- endif -%}\n",
        "    {{ '<start_of_turn>' + role + '\\n' + (first_user_prefix if loop.first else \"\") }}\n",
        "    {%- if message['content'] is string -%}\n",
        "        {{ message['content'] | trim }}\n",
        "    {%- elif message['content'] is iterable -%}\n",
        "        {%- for item in message['content'] -%}\n",
        "            {%- if item['type'] == 'image' -%}\n",
        "                {{ '<start_of_image>' }}\n",
        "            {%- elif item['type'] == 'text' -%}\n",
        "                {{ item['text'] | trim }}\n",
        "            {%- endif -%}\n",
        "        {%- endfor -%}\n",
        "    {%- else -%}\n",
        "        {{ raise_exception(\"Invalid content type\") }}\n",
        "    {%- endif -%}\n",
        "    {{ '<end_of_turn>\\n' }}\n",
        "{%- endfor -%}\n",
        "{%- if add_generation_prompt -%}\n",
        "    {{'<start_of_turn>model\\n'}}\n",
        "{%- endif -%}\n",
        "\"\"\"\n",
        "\n",
        "# Функция для применения шаблона чата\n",
        "def apply_chat_template(examples):\n",
        "    texts = [tokenizer.apply_chat_template(messages, tokenize=False) for messages in examples[\"messages\"]]\n",
        "    return {\"text\": texts}\n",
        "\n",
        "# Применяем функцию к датасету\n",
        "dataset = dataset.map(apply_chat_template, batched=True)"
      ],
      "metadata": {
        "id": "IVt_e5MMEULo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset[0][\"text\"])"
      ],
      "metadata": {
        "id": "btzupe4HEYJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95_Nn-89DhsL"
      },
      "outputs": [],
      "source": [
        "from trl import SFTTrainer, SFTConfig\n",
        "trainer = SFTTrainer(\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    train_dataset = dataset,\n",
        "    eval_dataset = None, # Can set up evaluation!\n",
        "    args = SFTConfig(\n",
        "        dataset_text_field = \"text\",\n",
        "        per_device_train_batch_size = 4,\n",
        "        gradient_accumulation_steps = 2, # Use GA to mimic batch size!\n",
        "        warmup_steps = 5,\n",
        "        num_train_epochs = 1, # Set this for 1 full training run.\n",
        "        learning_rate = 5e-5, # Reduce to 2e-5 for long training runs\n",
        "        logging_steps = 1,\n",
        "        optim = \"adamw_8bit\",\n",
        "        weight_decay = 0.01,\n",
        "        lr_scheduler_type = \"linear\",\n",
        "        seed = 3407,\n",
        "        report_to = \"none\", # Use this for WandB etc\n",
        "    ),\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqxqAZ7KJ4oL"
      },
      "outputs": [],
      "source": [
        "trainer_stats = trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "upcOlWe7A1vc"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"gemma-3\")\n",
        "tokenizer.save_pretrained(\"gemma-3\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}